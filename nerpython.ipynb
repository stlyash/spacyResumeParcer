{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\""
      ],
      "metadata": {
        "id": "qhin5twXqOIV"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "51_k3UbkjE_y"
      },
      "outputs": [],
      "source": [
        "!pip install spacy_transformers\n",
        "\n",
        "!pip install -U spacy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "import spacy_transformers\n",
        "from spacy.tokens import DocBin\n",
        "from tqdm import tqdm\n",
        "import json"
      ],
      "metadata": {
        "id": "8Knc4hJNlqWQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/stlyash/spacyResumeParcer.git"
      ],
      "metadata": {
        "id": "NLzmkNBmmBeD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "base config file is downloaded from https://spacy.io/usage/training/#quickstart"
      ],
      "metadata": {
        "id": "-8zQo-1tGX-e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cv_data = json.load(open('/content/spacyResumeParcer/data/training/train_data.json'))"
      ],
      "metadata": {
        "id": "l1Hpi0yGm9YB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(cv_data)"
      ],
      "metadata": {
        "id": "7Ndv0d5inEm3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy init fill-config /content/spacyResumeParcer/data/training/base_config.cfg /content/spacyResumeParcer/data/training/config.cfg"
      ],
      "metadata": {
        "id": "W6E_W15XnTZ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy import e\n",
        "def get_spacy_doc (file, data):\n",
        "  nlp = spacy.blank('en')\n",
        "  db = DocBin()\n",
        "\n",
        "  for text, annot in tqdm (data):\n",
        "    doc = nlp.make_doc(text) \n",
        "    annot =  annot['entities']\n",
        "    ents = []\n",
        "    entity_indices = []\n",
        "    for start, end, label in annot:\n",
        "      skip_entity = False\n",
        "      for idx in range(start,end):\n",
        "        if idx in entity_indices:\n",
        "          skip_entity = True\n",
        "          break\n",
        "      if skip_entity == True:\n",
        "        continue\n",
        "\n",
        "      entity_indices = entity_indices + list(range(start,end))\n",
        "\n",
        "      try:\n",
        "        span = doc.char_span(start, end, label = label, alignment_mode = 'strict')\n",
        "      except:\n",
        "        continue\n",
        "      \n",
        "      if span is None:\n",
        "        err_data = str([start,end]) + \"    \" + str(text) + \"\\n\"\n",
        "      else:\n",
        "        ents.append(span)\n",
        "\n",
        "    try:\n",
        "      doc.ents = ents\n",
        "      db.add(doc)\n",
        "    except:\n",
        "      pass\n",
        "      \n",
        "  return db"
      ],
      "metadata": {
        "id": "WM6eTavbqi3n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train,test = train_test_split(cv_data,test_size = 0.3)\n",
        "len(train),len(test)"
      ],
      "metadata": {
        "id": "5g3TnqIYv4Rw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file  = open('error.txt','w')\n",
        "db = get_spacy_doc(file,train)\n",
        "db.to_disk('train_data.spacy')\n",
        "\n",
        "db = get_spacy_doc(file,test)\n",
        "db.to_disk('test_data.spacy')\n",
        "file.close()"
      ],
      "metadata": {
        "id": "4CWs4MbawTdZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy train /content/spacyResumeParcer/data/training/config.cfg --output ./output --paths.train ./train_data.spacy --paths.dev ./test_data.spacy --gpu-id 0"
      ],
      "metadata": {
        "id": "-pyoSObxwyb3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing Model"
      ],
      "metadata": {
        "id": "_KFPmvoIyQdV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spacy-transformers"
      ],
      "metadata": {
        "id": "lflLGJic0K61"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load('/content/output/model-best')"
      ],
      "metadata": {
        "id": "3jAPWxv5yO8Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install docx2txt\n",
        "!pip install PyPDF2"
      ],
      "metadata": {
        "id": "xKATeQLMDqpw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import docx2txt     # to convert docx files into text\n",
        "import PyPDF2       # to convert pdf files into text"
      ],
      "metadata": {
        "id": "E-UMddf-DpIb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reading and Analyzing Bulk Resume(s)"
      ],
      "metadata": {
        "id": "AZAApTvgNBrR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Keywords to search in the resume\n",
        "keyString = \"C++,C,Python,Java,Excel,MatLab\"\n",
        "keylist = keyString.split(',')\n",
        "keylen = len(keylist)"
      ],
      "metadata": {
        "id": "Stdi0zIxOz9R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os  \n",
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "pathinp = r'/content/inputFolder'\n",
        "pathparent = r'..'\n",
        "os.chdir(pathinp)\n",
        "\n",
        "fileList = os.listdir()\n",
        "\n",
        "fileScore = []\n",
        "degree = []\n",
        "fileLen = len(fileList)\n",
        "nameList = []\n",
        "foundKeywordList = []\n",
        "emailList = []\n",
        "files = 0\n",
        "indicator = 0\n",
        "for fileName in fileList:\n",
        "\n",
        "    # Printing percentage of files processed\n",
        "    files+=1\n",
        "    if files%(fileLen//4) == 0:\n",
        "        indicator+=25\n",
        "        print(\"Processed {}%\".format(indicator))\n",
        "\n",
        "    # Reading text from a file into an array\n",
        "    keyfound = 0\n",
        "    text=''\n",
        "    try:\n",
        "        resume = docx2txt.process(fileName)\n",
        "        for i in resume:\n",
        "                if i == '\\n':\n",
        "                    text += ' '\n",
        "                else:\n",
        "                    text+=i\n",
        "    except:\n",
        "        file = open(fileName,'rb')\n",
        "        reader=PyPDF2.PdfReader(file)\n",
        "        for pg in range(len(reader.pages)):\n",
        "            page = reader.pages[pg]\n",
        "            tex = page.extract_text()\n",
        "            for i in tex:\n",
        "                if type(i)==str and i == '\\n':\n",
        "                    text += ' '\n",
        "                elif type(i)==str:\n",
        "                    text+=i\n",
        "        file.close()\n",
        "        \n",
        "    textRaw = text\n",
        "    textOriginal = text.split(' ')\n",
        "\n",
        "    # Searching for email\n",
        "    regex = r\"([a-zA-Z0-9._-]+@[a-zA-Z0-9._-]+\\.[a-zA-Z0-9_-]+\\.[a-zA-Z0-9_-]+) | ([a-zA-Z0-9._-]+@[a-zA-Z0-9._-]+\\.[a-zA-Z0-9_-]+)\"\n",
        "    matches = re.search(regex, text)\n",
        "    canEmail = ''\n",
        "    try:\n",
        "        canEmail = str((matches.group()).strip())\n",
        "    except:\n",
        "        canEmail = \"Not Found\"\n",
        "    emailList.append(canEmail)\n",
        "\n",
        "    # Searching Names\n",
        "    doc = nlp(text)\n",
        "    name = ''\n",
        "    for ent in doc.ents:\n",
        "      if str(ent.label_) == \"Name\":\n",
        "        name = str(ent.text)\n",
        "    nameList.append(name)\n",
        "\n",
        "    # Searching required keywords\n",
        "    found_skills = ''\n",
        "    for ent in doc.ents:\n",
        "      if str(ent.label_) == \"Skills\":\n",
        "        found_skills = str(ent.text)\n",
        "    foundKeywords=''\n",
        "    for ky in keylist:\n",
        "        if found_skills.find(ky) != -1:\n",
        "            keyfound += 1\n",
        "            foundKeywords += ky +', '\n",
        "    foundKeywordList.append(foundKeywords)\n",
        "    fileScore.append(keyfound*100/keylen)"
      ],
      "metadata": {
        "id": "jOgFdpYCNI0y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas --upgrade"
      ],
      "metadata": {
        "id": "Ohz8-9h8VcDr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Making a dataframe and exporting it to a csv\n",
        "os.chdir(pathparent)\n",
        "df = pd.DataFrame({\"File Names\" : fileList, r\"% Keywords Matched\" : fileScore,\"Keywords Found\":foundKeywordList,\"Name\":nameList,\"Email\":emailList})\n",
        "df=df[df[r\"% Keywords Matched\"]!=0]\n",
        "df = df.sort_values(r\"% Keywords Matched\",ascending=False)\n",
        "print(df)\n",
        "df.to_csv(\"/content/spacyResumeParcer/resumeResult.csv\", index=False)\n",
        "print(\"---  %s files scanned ---\" % (fileLen))"
      ],
      "metadata": {
        "id": "wVbNEJo4To2q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing a single Resume"
      ],
      "metadata": {
        "id": "gtsq2l54W-F3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "text = ''\n",
        "fileName = '/content/inputFolder/2.pdf'\n",
        "try:\n",
        "    resume = docx2txt.process(fileName)\n",
        "    for i in resume:\n",
        "            if i == '\\n':\n",
        "                text += ' '\n",
        "            else:\n",
        "                text+=i\n",
        "except:\n",
        "    file = open(fileName,'rb')\n",
        "    reader=PyPDF2.PdfReader(file)\n",
        "    for pg in range(len(reader.pages)):\n",
        "        page = reader.pages[pg]\n",
        "        tex = page.extract_text()\n",
        "        for i in tex:\n",
        "            if type(i)==str and i == '\\n':\n",
        "                text += ' '\n",
        "            elif type(i)==str:\n",
        "                text+=i\n",
        "    file.close()\n",
        "text"
      ],
      "metadata": {
        "id": "3k4DIc_NEAoe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Displaying Texts and Labels of Entities\n",
        "doc = nlp(text)\n",
        "for ent in doc.ents:\n",
        "  print(ent.text,\"  :  \",ent.label_)"
      ],
      "metadata": {
        "id": "XXkfgZWg0nQy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}